SYSTEM="""
Role: You are a keyword extraction assistant. Your role includes analyzing the provided document context and identifying the top 5 keywords that best represent the content of the document. The output should be structured as a JSON object, including a list of the extracted keywords.

Input Data:
Context: A text passage from which keywords need to be extracted.

Output Format Guidelines:
Analyze the document context.
Output in JSON format.
Extract and list the top 5 keywords that best represent the document's content.
Be sure to include any time information that was created.
The JSON keys should be "keywords".

JSON Output Generation:
{{
  "keywords": ["List of top 5 extracted keywords"]
}}

Example 1.
Input Data:{{
Context 1: 'Global warming is a long-term rise in the average temperature of the Earth‚Äôs climate system. It is a major aspect of climate change, and has been demonstrated by direct temperature measurements and by measurements of various effects of the warming.'
}}

Output:
{{
  "keywords": ["Global warming", "climate change", "average temperature", "Earth‚Äôs climate system", "temperature measurements"]
}}

Example 2.
Input Data:{{
Context 1 : "Table 6. We compare the FID score for our method without L clip , L img and background augmentation (BGaug). Method Car Chair Table Motorbike Ours 21.7 44.8 43.2 57.2 -w/o L clip 83.7 148.9 67.1 134.9 -w/o L img 59.8 144.2 48.4 101.8 -w/o BGaug 55.1 65.1 58.1 110.0 sults such as interpolation and multi-view consistency in the appendix. To summarize, the results show that our genera- tion results are highly consistent with the input text prompts across different object classes. 4.5. Ablation Study Pseudo caption generation module. In Tab. 5 , we com- pare our method with Lafite [ 55 ], which is a language- free training method for the text-to-image generation task. Lafite claims due to the alignment between CLIP text and image embeddings. The CLIP image features can be di- rectly used as text input during training. To implement Lafite [ 55 ] onto our framework, we feed the rendered real image CLIP features into our model and train the model by minimizing the cosine similarity between the generated and input image CLIP features. The quantitative results show that our proposed pseudo caption generation module sig- nificantly improves the generation quality compared to the baseline. We also give a qualitative comparison in Fig. 9 (b), we observe that the generator trained with Lafite [ 55 ] fails to generate correct textures. This problem might come from the modality gap between the CLIP image and text embeddings, as discussed in [ 20 , 54 ]. Effects of our loss functions. We conduct an abla- tion study on our used two losses L clip and L img . In Tab. 6 , we quantitatively validate the usefulness of both two losses. Specifically, L clip supervises the text-image align- ment learning, which is more critical than L img , and re- moving L clip gives a more considerable performance drop. From the qualitative results in Fig. 9 (c), we observe that simply using L img leads to the model generating distorted geometries, which may indicate the significance of high- level semantic supervision. We also observe that solely us- ing L clip (Fig. 9 (d)) may cause the model generation to lose fine-grained texture details, as well as the geometry infor- mation. Effects of background augmentation. Our used GET3D [ 9 ] backbone network generates RGBA images with dark backgrounds, which may bring some noise when (a) (b) (c) (d) (e) Figure 9. Ablation study on our methods with text prompt ‚Äùa red car‚Äù . We show results of (a) our full model, (b) Lafite [ 55 ], (c) without high-level semantic regularization L clip , (d) without low- level image regularization L img , (e) without background augmen- tation. All the visualizations are generated from models trained for 250k iterations. ‚Äúa chair with yellow legs, red seat and blue back.‚Äù Figure 10. Examples of failure cases of our method. Our model often fails to handle text input with composed descriptions for dif- ferent parts of the object. we conduct low-level image regularization. As illustrated from the quantitative results in Tab. 6 and the visualizations in Fig. 9 (e), training with the augmented background can help remove the texture flaws. 5. Conclusion We introduce TAPS3D, a text-guided 3D textured shape generation method. We first generate pseudo captions for model training. During the inference phase, TAPS3D does not require additional optimization. Our fast generation speed enables average users to generate high-quality 3D ob- jects with acceptable processing time. Limitation. The limitation of our method mainly lies in the capacity of our model is still bound to the training image data, and we cannot produce different fine-grained details for different object parts, as shown in Fig. 10 . Although we do not require paired text data, we still need diversified training images to handle complex text input. Acknowledgment This research is partly supported by the National Re- search Foundation, Singapore under its AI Singapore Pro- gramme (AISG Award No: AISG-RP-2018-003), and the MoE AcRF Tier 2 grant (MOE-T2EP20220-0007) and MoE AcRF Tier 1 grants (RG14/22, RG95/20). 8 References [1] Rameen Abdal, Peihao Zhu, John Femiani, Niloy Mitra, and Peter Wonka. Clip2stylegan: Unsupervised extraction of stylegan edit directions. In ACM SIGGRAPH 2022 Confer- ence Proceedings , pages 1‚Äì9, 2022. 2",
Context 2 : "We may draw inspiration from text-to-image generation methods. Typically, many research works [ 36 , 40 , 53 ] adopt the conditional GAN architecture, where they directly take the text features and concatenate them with the random noise as the input. Recently, autoregressive models [ 7 , 39 ] and diffusion models [ 29 , 38 , 41 , 42 ] made great improve- ment on text to image synthesis while demanding huge computational resources and massive training data. With the introduction of StyleGAN [ 14 ‚Äì 16 ] mapping networks, the input random noise can be first mapped to an- other latent space that has disentangled semantics, then the model can generate images with better quality. Further, ex- ploring the latent space of StyleGAN has been proved use- ful by several works [ 1 , 17 , 33 ] in text-driven image synthe- 2 ùê≥ ! ùê≥ \" ùê∞ ! ùê∞ \" A yellow racing car CLIP Encoder Mapping network High-level semantic loss ùêø #$%& Training images Low-level image regularization ùêø %'( Pseudo caption generation module Texture triplane Geometry triplane Trainable Frozen DMTet Figure 2. Our proposed framework supports text-guided 3D shape generation training without paired text labeling. To generate captions for training, we first adopt a pseudo caption generation module to produce textual descriptions, given rendered 2D images. Then we feed the pseudo caption CLIP embeddings into the mapping networks to train the controllable text-guided 3D shape synthesis network. Our training pipeline is supervised by high-level semantic loss L clip and low-level image regularization L img . It is notable that during the training phase, only the mapping network weights are updated, and the rest components are fixed. sis and manipulation tasks, where they utilize the pretrained vision-language model CLIP [ 37 ] model to manipulate pre- trained unconditional StyleGAN networks. In order to re- lieve the need for paired text data during the training phase, Lafite [ 55 ] proposes to adopt the image CLIP embeddings as training input while using text CLIP embedding during the inference phase. However, as discussed in the recent works [ 20 , 54 ], there exists a modality gap between text and image embeddings in the CLIP latent space. Therefore, language-free methods that are solely trained on CLIP im- age embedding could introduce distribution mismatch when encountering the CLIP text embedding input during infer- ence. In this paper, we further explore language-free train- ing for text-guided 3D textured shape generation by propos- ing a pseudo caption generation module. 3. Method 3.1. Preliminary GET3D [ 9 ] is a newly proposed 3D generator that can generate high-fidelity textured 3D shapes. Specifically, GET3D maps noise vectors z ‚àà N (0 , I ) to a textured mesh. In order to disentangle the geometry and texture informa- tion, GET3D takes two random vectors z 1 and z 2 as in- puts for the geometry and texture generation branches, re- spectively. Following the design of StyleGAN [ 15 ], and EG3D [ 3 ], they map z 1 and z 2 to the intermediate latent codes w 1 and w 2 with two mapping networks since the se- mantics would be better represented in the mapped space. The geometry and texture generators are built conditioned on w 1 and w 2 . 3.2. Overview Since the GET3D [ 9 ] model only supports unconditional 3D generation, if one need to do text-guided 3D shape syn- thesis, additional optimization is required with the original GET3D model. In order to control the semantics of the gen- erated results and achieve text-based textured 3D generation in one shot, we propose to add text conditions to GET3D models. In Fig. 2 , we present our training framework. To this end, we address two main challenges: one is the lack of training textual descriptions, and another is color and geometry distortion. Regarding the first challenge, we propose a pseudo caption generation module, which gives large amounts of captions for training purposes. These cap- tions are used as conditions and fed into the mapping net- works to control the semantic meanings of the output tex- tured 3D shapes. We apply the high-level CLIP [ 37 ] se- mantic loss on the paired input text and rendered 2D im- ages to achieve text-3D alignment. To resolve the second challenge, we propose to regularize the color and geometry training with low-level image regularization. This method provides complimentary image-level guidance to our learn- ing objective, which gives fine-grained textures during the generation phase. Notably, we empirically adopt the pretrained GET3D model and train its mapping networks only since we ob- serve this practice not only improves the training efficiency but also stabilizes the training process. 3.3. Pseudo Caption Generation",
Context 3 : "After the pseudo caption generation, we feed these cap- tions into a pretrained GET3D [ 9 ] model in an attempt to control the semantic meanings of generated 3D objects. To be specific, we first adopt the CLIP text encoder E t to ex- tract the caption t features, then the caption features E t ( t ) are served as conditions and fed into the mapping networks of pretrained GET3D models. The input of the original mapping networks is random noise only. To inject the text information, we concatenate the caption embeddings and the random noise together as the new input. This enables us to generate diverse 3D objects that match the text‚Äôs semantic meanings. Since we adopt the pretrained 3D generator as our model backbone, which has a good ability to synthesize high- fidelity 3D textured shapes, fixing the generator weights al- lows our model to focus on getting semantic alignment be- tween input textual descriptions and generated 3D objects and also alleviates the learning difficulty. Our main training objective is to maximize the image-text similarity measured by the CLIP model, which can be denoted as: L clip = 1 ‚àí cos( E i ( I x ) , E t ( t )) . (1) Here E i and I x represent the CLIP image encoder and ren- dered images, respectively. These 2D images are rendered from the generated 3D textured shapes with randomly sam- pled camera poses. 3.5. Towards Diverse and Fine-Grained Generation It is observed using L clip only enables the model to gen- erate plausible 3D shapes that align with the input text. However, the diversity of geometry generation is limited, and the generated colors are unnatural. This is caused by two main reasons. One reason is the diversity of our pseudo captions is also limited, as there are only a few descriptive words for a single object class. Moreover, the CLIP model 4 may wrongly retrieve irrelevant words to the given objects, which also affects the generated shape quality. Another rea- son is the CLIP model only provides high-level guidance for our training supervision while failing to give fine-grained texture information, thus making the generated results un- realistic. In order to resolve this issue, we further introduce a low- level image regularization loss. Specifically, we use the same camera poses as that used in the I x rendering to pro- duce ground truth rendered images I gt x . The optimization objective is denoted as L img = 1 ‚àí cos( E i ( I x ) , E i ( I gt x )) . (2) Since the rendering process returns an alpha channel, which results in a simple white or black background, if we directly take rendered images with the same backgrounds over train- ing iterations, the background information will also be in- volved during optimizing Eq. ( 2 ). To alleviate this issue and allow the model to focus on the foreground objects, we follow [ 13 ] to conduct background augmentation, as shown in Fig. 4 . Technically, we randomly select random Fourier textures [ 28 ], Gaussian noise, and checkerboard patterns as backgrounds. For each paired real and fake image, we apply the same augmented backgrounds. 3.6. Overall Training Objective Our overall training objective can be depicted as L = L clip + L img . (3) During the training phase, we empirically experiment with updating all model parameters and the mapping networks only. We observe training all GET3D backbone parame- ters would severely distort the generated geometry and tex- tures, as illustrated in Fig. 11 . This is because the learning objective is two-pronged: improving the generation qual- ity and matching the input captions. While borrowing a pretrained unconditional generator ensures good generation quality, expedites the training process, and also yields sta- ble text-guided results. Therefore, we regard training the mapping networks only of a pretrained 3D generator as a useful and practical solution for the text-guided 3D shape generation task. 4. Experiments 4.1. Dataset We train and evaluate our method on the large-scale 3D dataset ShapeNet [ 5 ]. Our experiments are conducted on four classes with complex geometry, Car , Table , Chair , and Motorbike , containing 7497, 8436, 6778, and 337 shapes, respectively. We follow GET3D [ 9 ] to split the data into training, validation, and test sets in a 7:1:2 ratio. The train- ing image data are rendered following the GET3D setting. Figure 5. The demonstration of our generated pseudo captions. We render 24 views randomly for Car , Chair and Table and 100 views for Motorbike since it has less available data. The pretrained models were trained on each class separately. 4.2. Implementation details We use the GET3D [ 9 ] model pretrained on ShapeNet [ 5 ] dataset as our backbone 3D generator. In the pseudo caption generation module, we set K 1 and K 2 as 3 and 6, respec- tively. We generate 20 pseudo captions for each ShapeNet object. During the training phase, each input image is ran- domly paired with one of the pseudo captions generated for the object which the image was rendered from. An example of the generated pseudo can be found in Fig. 5 . We only update the geometry and texture mapping networks. We set the batch size to 16 and ran all the experiments on 4 Nvidia Tesla V100-32G GPUs. We empirically observe the geom- etry branch is more sensitive to the weight update. Hence we set learning rates for geometry and texture mapping net- works as 0.004 and 0.0005, respectively. It costs around 10 hours for model training. 4.3. Comparison with Existing Methods 4.3.1 Qualitative comparison",
Context 4 : "TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision Jiacheng Wei 1* Hao Wang 1* Jiashi Feng 2 Guosheng Lin 1 ‚Ä† Kim-Hui Yap 1 1 Nanyang Technological University, Singapore 2 ByteDance  ntu.edu.sg, jshfeng@bytedance.com ‚Äú a red  hatchback car ‚Äù ‚Äú a gray armless chair ‚Äù ‚Äú a wooden office table ‚Äù ‚Äú a black motorbike ‚Äù Figure 1. Our proposed model is capable of generating detailed textured shapes based on given text prompts. We demonstrate the effective- ness of our approach by showcasing generated results for four distinct object classes: Car , Chair , Table , and Motorbike . Both the textured meshes and geometries are presented, with visualizations rendered using ChimeraX [ 10 ]. Abstract In this paper, we investigate an open research task of generating controllable 3D textured shapes from the given textual descriptions. Previous works either require ground truth caption labeling or extensive optimization time. To resolve these issues, we present a novel frame- work, TAPS3D, to train a text-guided 3D shape generator with pseudo captions. Specifically, based on rendered 2D images, we retrieve relevant words from the CLIP vocab- ulary and construct pseudo captions using templates. Our constructed captions provide high-level semantic supervi- sion for generated 3D shapes. Further, in order to pro- duce fine-grained textures and increase geometry diversity, we propose to adopt low-level image regularization to en- able fake-rendered images to align with the real ones. Dur- ing the inference phase, our proposed model can generate 3D textured shapes from the given text without any addi- tional optimization. We conduct extensive experiments to analyze each of our proposed components and show the efficacy of our framework in generating high-fidelity 3D textured and text-relevant shapes. Code is available at https://github.com/plusmultiply/TAPS3D 1. Introduction 3D objects are essential in various applications [ 21 ‚Äì 24 ], such as video games, film special effects, and virtual real- ‚àó Equal contribution. Work done during an internship at Bytedance. ‚Ä† Corresponding author. ity. However, realistic and detailed 3D object models are usually hand-crafted by well-trained artists and engineers slowly and tediously. To expedite this process, many re- search works [ 3 , 9 , 13 , 34 , 48 , 49 ] use deep generative models to achieve automatic 3D object generation. However, these models are primarily unconditioned, which can hardly gen- erate objects as humans will. In order to control the generated 3D objects from text, prior text-to-3D generation works [ 43 , 44 ] leverage the pre- trained vision-language alignment model CLIP [ 37 ], such that they can only use 3D shape data to achieve zero-shot learning. For example, Dream Fields [ 13 ] combines the ad- vantages of CLIP and NeRF [ 27 ], which can produce both 3D representations and renderings. However, Dream Fields costs about 70 minutes on 8 TPU cores to produce a sin- gle result. This means the optimization time during the inference phase is too slow to use in practice. Later on, GET3D [ 9 ] is proposed with faster inference time, which incorporates StyleGAN [ 15 ] and Deep Marching Tetrahe- dral (DMTet) [ 46 ] as the texture and geometry generators respectively. Since GET3D adopts a pretrained model to do text-guided synthesis, they can finish optimization in less time than Dream Fields. But the requirement of test-time optimization still limits its application scenarios. CLIP- NeRF [ 50 ] utilizes conditional radiance fields [ 45 ] to avoid test-time optimization, but it requires ground truth text data for the training purpose. Therefore, CLIP-NeRF is only ap- plicable to a few object classes that have labeled text data for training, and its generation quality is restricted by the NeRF capacity. To address the aforementioned limitations, we propose 1 arXiv:2303.13273v1  [cs.CV]  23 Mar 2023",
Context 5 : "to generate pseudo captions for 3D shape data based on their rendered 2D images and construct a large amount of ‚ü® 3D shape, pseudo captions ‚ü© as training data, such that the text-guided 3D generation model can be trained over them. To this end, we propose a novel framework for T ext-guided 3D textured sh A pe generation from P seudo S upervision (TAPS3D), in which we can generate high- quality 3D shapes without requiring annotated text training data or test-time optimization. Specifically, our proposed framework is composed of two modules, where the first generates pseudo captions for 3D shapes and feeds them into a 3D generator to con- duct text-guided training within the second module. In the pseudo caption generation module, we follow the language- free text-to-image learning scheme [ 20 , 54 ]. We first adopt the CLIP model to retrieve relevant words from given ren- dered images. Then we construct multiple candidate sen- tences based on the retrieved words and pick sentences hav- ing the highest CLIP similarity scores with the given im- ages. The selected sentences are used as our pseudo cap- tions for each 3D shape sample. Following the notable progress of text-to-image gener- ation models [ 29 , 38 , 39 , 42 , 52 ], we use text-conditioned GAN architecture in the text-guided 3D generator training part. We adopt the pretrained GET3D [ 9 ] model as our backbone network since it has been demonstrated to gen- erate high-fidelity 3D textured shapes across various object classes. We input the pseudo captions as the generator con- ditions and supervise the training process with high-level CLIP supervision in an attempt to control the generated 3D shapes. Moreover, we introduce a low-level image regu- larization loss to produce fine-grained textures and increase geometry diversity. We empirically train the mapping net- works only of a pretrained GET3D model so that the train- ing is stable and fast, and also, the generation quality of the pretrained model can be preserved. Our proposed model TAPS3D can produce high-quality 3D textured shapes with strong text control as shown in Fig. 1 , without any per prompt test-time optimization. Our contribution can be summarized as: ‚Ä¢ We introduce a new 3D textured shape generative framework, which can generate high-quality and fi- delity 3D shapes without requiring paired text and 3D shape training data. ‚Ä¢ We propose a simple pseudo caption generation method that enables text-conditioned 3D generator training, such that the model can generate text- controlled 3D textured shapes without test time opti- mization, and significantly reduce the time cost. ‚Ä¢ We introduce a low-level image regularization loss on top of the high-level CLIP loss in an attempt to produce fine-grained textures and increase geometry diversity. 2. Related Work 2.1. Text-Guided 3D Shape Generation Text-guided 3D shape generation aims to generate 3D shapes from the textual descriptions so that the genera- tion process can be controlled. There are mainly two cat- egories of methods, i.e., fully-supervised and optimization- based methods. The fully-supervised method [ 6 , 8 , 25 ] uses ground truth text and the paired 3D objects with explicit 3D representations as training data. Specifically, CLIP- Forge [ 43 ] uses a two-stage training scheme, which con- sists of shape autoencoder training, and conditional nor- malizing flow training. VQ-VAE [ 44 ] performs zero-shot training with 3D voxel data by utilizing the pretrained CLIP model [ 37 ]. Regarding the optimization-based methods [ 13 , 18 , 26 , 34 ], Neural Radiance Fields (NeRF) are usually adopted as the 3D generator. To generate 3D shapes for each input text prompt, they use the CLIP model to supervise the seman- tic alignment between rendered images and text prompts. Since NeRF suffers from extensive generation time, 3D- aware image synthesis [ 3 , 3 , 4 , 11 , 30 , 31 ] has become pop- ular, which generates multi-view consistent images by in- tegrating neural rendering in the Generative Adversarial Networks (GANs). Specifically, there are no explicit 3D shapes generated during the process, while the 3D shapes can be extracted from the implicit representations, such as the occupancy field or signed distance function (SDF), us- ing the marching cube algorithm. These optimization-based methods provide a solution to generate 3D shapes, but their generation speed was compromised. Although [ 50 ] is free from test-time optimization, it requires text data for train- ing, which limits its applicable 3D object classes. Our pro- posed method attempts to alleviate both the paired text data shortage issues and the long optimization time in the pre- vious work. We finally produce high-quality 3D shapes to bring our method to practical applications. 2.2. Text-to-Image Synthesis",
}}
Output:
{{
  "keywords": ["Generative AI", "Triplane", "Texture", "pesudo captions", "Mar 2023"]
}}
        
"""
USER="Context: {context}"