{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chois\\AppData\\Local\\Continuum\\anaconda3\\envs\\refeat_ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from utils import add_api_key\n",
    "add_api_key()\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from models.llm.agent.conversational_chat_agent import ConversationalChatAgent\n",
    "from models.tools import WebSearchTool, DBSearchTool\n",
    "from models.llm.agent.custom_streming_callback import CustomStreamingStdOutCallbackHandler\n",
    "\n",
    "class ChatAgent:\n",
    "    def __init__(self):\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.0, streaming=True, seed=42)\n",
    "        tools = [WebSearchTool(), DBSearchTool()]\n",
    "        agent = ConversationalChatAgent.from_llm_and_tools(llm=llm, tools=tools)\n",
    "        self.queue = [] # TODO: 나중에 backend에서 주면 삭제\n",
    "        self.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "        self.streaming_callback = CustomStreamingStdOutCallbackHandler(queue=self.queue)\n",
    "    \n",
    "    def run(self, query, chat_history: List[List[str]]):\n",
    "        input_dict = self.parse_input(query, chat_history)\n",
    "        result = self.agent_executor.run(input_dict, callbacks=[self.streaming_callback])\n",
    "        print(result)\n",
    "        return result\n",
    "\n",
    "    def parse_input(self, query, chat_history):\n",
    "        parsed_chat_history = []\n",
    "        for human, assistant in chat_history:\n",
    "            parsed_chat_history.append(HumanMessage(content=human))\n",
    "            parsed_chat_history.append(AIMessage(content=assistant))\n",
    "        return {\"input\": query, \"chat_history\": parsed_chat_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! seed is not default parameter.\n",
      "                    seed was transferred to model_kwargs.\n",
      "                    Please confirm that seed is what you intended.\n"
     ]
    }
   ],
   "source": [
    "chat_agent = ChatAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Database Search\",\n",
      "    \"action_input\": \"Cross-lingual Language Model Pretraining bleu score\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mresult1. Cross-lingual Language Model Pretraining Guillaume Lample∗ Facebook AI Research Sorbonne Universit´es glample@fb.com Alexis Conneau∗ Facebook AI Research Universit´e Le Mans aconneau@fb.com 9 1 0 2 n a J 2 2 ] L C . s c [ 1 v 1 9 2 7 0 . 1 0 9 1 : v i X r a Abstract Recent studies have demonstrated the ef- ﬁciency of generative pretraining for En- In glish natural language understanding. this work, we extend this approach to mul- tiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual lan- guage models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art re- sults on cross-lingual classiﬁcation, unsu- pervised and supervised machine transla- tion. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine trans- lation, we obtain 34.3 BLEU on WMT’16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperform- ing the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.\n",
      "\n",
      "\n",
      "result2. Table 2: Results on unsupervised MT. BLEU scores on WMT’14 English-French, WMT’16 German-English and WMT’16 Romanian- English. For our results, the ﬁrst two columns indicate the model used to pretrain the encoder “ - ” means the model was and the decoder. randomly initialized. EMB corresponds to pretraining the lookup table with cross-lingual embeddings, CLM and MLM correspond to pretraining with models trained on the CLM or MLM objectives. tion, which results in 9 different settings. We then follow Lample et al. (2018b) and train the model with a denoising auto-encoding loss along with an online back-translation loss. Results are reported in Table 2. We compare our approach with the ones of Lample et al. (2018b). For each language pair, we observe signiﬁcant improve- ments over the previous state of the art. We re- implemented the NMT approach of Lample et al. (2018b) (EMB), and obtained better results than reported in their paper. We expect that this is due to our multi-GPU implementation which uses In German-English, signiﬁcantly larger batches. our best model outperforms the previous unsuper- vised approach by more than 9.1 BLEU, and 13.3 BLEU if we only consider neural unsupervised approaches. Compared to pretraining only the lookup table (EMB), pretraining both the encoder and decoder with MLM leads to consistent signif- icant improvements of up to 7 BLEU on German- English. We also observe that the MLM objec- tive pretraining consistently outperforms the CLM one, going from 30.4 to 33.4 BLEU on English- French, and from 28.0 to 31.8 on Romanian- English. These results are consistent with the ones of Devlin et al. (2018) who observed a better gen-\n",
      "\n",
      "\n",
      "result3. Supervised machine translation In Table 3 we report the performance on Romanian-English WMT’16 for different supervised training conﬁg- urations: mono-directional (ro→en), bidirectional (ro↔en, a multi-NMT model trained on both en→ro and ro→en) and bidirectional with back- translation (ro↔en + BT). Models with back- translation are trained with the same monolin- gual data as language models used for pretraining. As in the unsupervised setting, we observe that pretraining provides a signiﬁcant boost in BLEU score for each conﬁguration, and that pretraining with the MLM objective leads to the best perfor- mance. Also, while models with back-translation have access to the same amount of monolingual data as the pretrained models, they are not able to generalize as well on the evaluation sets. Our bidirectional model trained with back-translation obtains the best performance and reaches 38.5 BLEU, outperforming the previous SOTA of Sen- nrich et al. (2016) (based on back-translation and ensemble models) by more than 4 BLEU. Low-resource language model In Table 4, we investigate the impact of cross-lingual language modeling for improving the perplexity of a Nepali language model. To do so, we train a Nepali lan- guage model on Wikipedia, together with addi- tional data from either English or Hindi. While Nepali and English are distant languages, Nepali and Hindi are similar as they share the same De- Training languages Nepali perplexity Cosine sim. L2 dist. SemEval’17 Nepali Nepali + English Nepali + Hindi Nepali + English + Hindi 157.2 140.1 115.6 109.3\n",
      "\n",
      "\n",
      "result4. tion, which results in 9 different settings. We then follow Lample et al. (2018b) and train the model with a denoising auto-encoding loss along with an online back-translation loss. Results are reported in Table 2. We compare our approach with the ones of Lample et al. (2018b). For each language pair, we observe signiﬁcant improve- ments over the previous state of the art. We re- implemented the NMT approach of Lample et al. (2018b) (EMB), and obtained better results than reported in their paper. We expect that this is due to our multi-GPU implementation which uses In German-English, signiﬁcantly larger batches. our best model outperforms the previous unsuper- vised approach by more than 9.1 BLEU, and 13.3 BLEU if we only consider neural unsupervised approaches. Compared to pretraining only the lookup table (EMB), pretraining both the encoder and decoder with MLM leads to consistent signif- icant improvements of up to 7 BLEU on German- English. We also observe that the MLM objec- tive pretraining consistently outperforms the CLM one, going from 30.4 to 33.4 BLEU on English- French, and from 28.0 to 31.8 on Romanian- English. These results are consistent with the ones of Devlin et al. (2018) who observed a better gen- Sennrich et al. (2016) ro → en ro ↔ en ro ↔ en + BT 33.9 28.4 28.5 34.4 - 31.5 31.5 37.0 - 35.3 35.6 38.5 Table 3: Results on supervised MT. BLEU scores on WMT’16 Romanian-English. The previous state-of-the-art of Sennrich et al. (2016) uses both back-translation and an ensemble model. ro ↔ en corresponds to models trained on both directions.\n",
      "\n",
      "\n",
      "result5. Table 1: Results on cross-lingual classiﬁcation accuracy. Test accuracy on the 15 XNLI languages. We report results for machine translation baselines and zero-shot classiﬁcation approaches based on cross-lingual sentence encoders. XLM (MLM) corresponds to our unsupervised approach trained only on monolingual corpora, and XLM (MLM+TLM) corresponds to our supervised method that leverages both monolingual and parallel data through the TLM objective. ∆ corresponds to the average accuracy. 5.3 Results and analysis In this section, we demonstrate the effectiveness of cross-lingual language model pretraining. Our ap- proach signiﬁcantly outperforms the previous state of the art on cross-lingual classiﬁcation, unsuper- vised and supervised machine translation. Cross-lingual classiﬁcation In Table 1, we eval- uate two types of pretrained cross-lingual en- coders: an unsupervised cross-lingual language model that uses the MLM objective on monolin- gual corpora only; and a supervised cross-lingual language model that combines both the MLM and the TLM loss using additional parallel data. Following Conneau et al. (2018b), we include two machine translation baselines: TRANSLATE- TRAIN, where the English MultiNLI training set is machine translated into each XNLI lan- guage, and TRANSLATE-TEST where every dev and test set of XNLI is translated to English. We report the XNLI baselines of Conneau et al. (2018b), the multilingual BERT approach of De- vlin et al. (2018) and the recent work of Artetxe and Schwenk (2018).\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "Thought:Cross-lingual Language Model Pretraining의 BLEU 점수는 34.3으로 WMT’16 German-English에서 9 BLEU 이상의 이전 최고점을 개선했으며, supervised machine translation에서는 WMT’16 Romanian-English에서 38.5 BLEU로 이전 최고 성능을 능가했습니다.\"\n",
      "}\n",
      "```\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Cross-lingual Language Model Pretraining의 BLEU 점수는 34.3으로 WMT’16 German-English에서 9 BLEU 이상의 이전 최고점을 개선했으며, supervised machine translation에서는 WMT’16 Romanian-English에서 38.5 BLEU로 이전 최고 성능을 능가했습니다.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Cross-lingual Language Model Pretraining의 BLEU 점수는 34.3으로 WMT’16 German-English에서 9 BLEU 이상의 이전 최고점을 개선했으며, supervised machine translation에서는 WMT’16 Romanian-English에서 38.5 BLEU로 이전 최고 성능을 능가했습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cross-lingual Language Model Pretraining의 BLEU 점수는 34.3으로 WMT’16 German-English에서 9 BLEU 이상의 이전 최고점을 개선했으며, supervised machine translation에서는 WMT’16 Romanian-English에서 38.5 BLEU로 이전 최고 성능을 능가했습니다.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_agent.run(\"데이터베이스에 저장된 Cross-lingual Language Model Pretraining bleu score\", [[\"안녕하세요\", \"무엇을 도와드릴까요?\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refeat_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
